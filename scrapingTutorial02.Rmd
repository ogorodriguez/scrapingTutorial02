---
title: "Scraping HTML Basic"
output:
  html_document:
    toc: yes
    toc_depth: 2
    toc_float: yes
  pdf_document:
    toc: yes
  word_document:
    toc: yes
urlcolor: blue
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE)

```


```{r, load-preliminary-libraries, include = FALSE}
library(dplyr)
library(ggplot2)
library(pander)
library(rvest)
library(XML)


```


# Basis of this tutorial.  Objectives

This tutorial is basic and will focus on the following tasks:

1. Importing Excel files found online
2. Reading HTML text
3. Reading HTML tables
4. Handling APIs

# 1. Importing Excel files found online

The first task is to download the csv of the federal agencies that supply data to the [Data.gov](https://www.data.gov/metrics)

This is not considered sometimes webscraping at all.

Since lots of data is stored in csv/excel or other tabular formula, a good starting point is to be able to import those from the sites that hosts them.

```{r, import-datagov-metrics-data-csv}

# the url for the online csv
url <- "https://s3.amazonaws.com/bsp-ocsit-prod-east-appdata/datagov/wordpress/agency-participation.csv"

# use read.csv to import
datagov <- read.csv(url, stringsAsFactors = FALSE)

# check the set has been read properly, check first rows
head(datagov)
```

```{r, import-datagov-metrics-data-csv-1}
# Check data summaries, etc.

glimpse(datagov)

```

As can be seen some entries will have to be formatted as factor (Organization.Type), and as Date (Last.Entry)

That was for importing csv files.  Now for importing Excel files.  There is not a base-R function, so a package must be included.  The package `gdata` for instance has a read.xls function. 

The objective is to download the excel file of the Fair Market Rents (FMR)

```{r, import-FMR-excel}

# Load the library that will handle the excel file.  In the tutorial they use gdata.  I will use xlsx since it can handle both xls and xlsx data.  I don't know if gdata handles xlsx.

library(gdata)


url <- "http://www.huduser.org/portal/datasets/fmr/fmr2015f/FY2015F_4050_Final.xls"
  
rents<- read.xls(url, perl = "C:/Perl64/bin/perl.exe")

head(rents)



```



Another common form of file storage is zip files.  For this we can use download.file() to get into our directory and use as desired.


```{r, import-zip-bureau-labor-stats}

# These are the diary of 2014.  The diary of 2016 is available as well.

url <- "http://www.bls.gov/cex/pumd/data/comma/diary14.zip"

# downloado .zip files and unzip

download.file(url, dest = "data/dataset.zip", mode = "wb")

# unzip in same folder.  The exdir must be the name of the folder where it will be unzipped.  Better do it ahead of time in the meantime.
unzip("data/dataset.zip", exdir = "data")

#Check the files within the folder

zipFiles <- list.files("data/diary14")

 # How many files?
length(zipFiles)

```

```{r, import-zip-bureau-labor-stats-1}
# List the files
zipFiles
```


If we know the file we want prior to unzipping we could extract without unzipping the whole set

```{r, import-zip-bureau-labor-stats-2}
zipData <- read.csv(unz("data/dataset.zip", "diary14/expd141.csv"))

glimpse(zipData)

```


```{r, import-zip-bureau-labor-stats-3}

head(zipData)

```

Sometimes the zip file is so big it is counterproductive to save it when all we need to work with is one of the internal files (or a group of them), the proposal is to temporarily save the zip, fetch the files we need and then discard the zip.

```{r, import-zip-bureau-labor-stats-4}

# Create a temp. file name
temp <- tempfile()

# Download the zip to the temp file name.  for url, see item 102
download.file(url, temp)

# Use unz() to extract the target file from temp.
zipData2 <- read.csv(unz(temp, "diary14/expd141.csv"))

# Remove the temp file via unlink()
unlink(temp)


# Check zipData2
glimpse(zipData2)


```

```{r, import-zip-bureau-labor-stats-5}
# Check the dataset
head(zipData2)
```


```{r, import-zip-bureau-labor-stats-6}
# Save it into the working dir
write.csv(zipData2, "data/zipData2.csv")

```

One thing is to detect all the links in a webpage using the XML package getHTMLLinks() function.

```{r, identify-links-in-webpage}

# Load the XML library
library(XML)

# The url to use is the Maryland State Board of Elections website
url <- "http://www.elections.state.md.us/elections/2012/election_data/index.html"
links <- getHTMLLinks(url)

# How many links were fetched?
length(links)


```

```{r, identify-links-in-webpage-1}
# Check links set
head(links)
```


Let's identify all of the links with the .csv extension

```{r, identify-links-in-webpage-csv}
# We will use regular expressions to identify those links that contain csv
# We will require the stingr package
library(stringr)

# extract the names of desired links with csv extension
linksData <- links[str_detect(links, ".csv")]

# How many were identified?
length(linksData)


```


```{r, identify-links-in-webpage-csv-10}
head(linksData, 10)
```

We will download only the first 10 csv files

```{r, identify-links-in-webpage-download-10-csv}

# We will use a for loop.  But first reduce linksData length to 10
linksData <- head(linksData, 10)

for (i in seq_along(linksData)) {
  # step 1: paste .csv portion of link to base URL of page
  url <- paste0("http://www.elections.state.md.us/elections/2012/election_data/",
                linksData[i])
  
  # setp 2: download .csv file and save as df
  df <- read.csv(url)
  
  # step 3: rename df
  assign(paste0("df", i), df)
}

```

Let's check if the 10 df sets exists


```{r, check-dfs}
sapply(paste0("df", 1:10), exists)

```

```{r, check-df8}

head(df8)

```

# Excercises 1

1. Import the following csv file directly from this url: https://bradleyboehmke.github.io/public/data/reddit.csv.


```{r, ex1.1-import-csv}

url <- "https://bradleyboehmke.github.io/public/data/reddit.csv"

reddit <- read.csv(url, stringsAsFactors = FALSE)

# Check the data set
glimpse(reddit)

```


```{r, ex1-import-csv-10}
# Let's save it to our data file
reddit <- write.csv(reddit, "data/reddit.csv")

```


2. Import the .xlsx file directly from this url: http://www.huduser.gov/portal/datasets/fmr/fmr2017/FY2017_4050_FMR.xlsx.

```{r, ex1.2-import-xlsx}

url <- "http://www.huduser.gov/portal/datasets/fmr/fmr2017/FY2017_4050_FMR.xlsx"

fmr2017 <- read.xls(url, perl = "C:/Perl64/bin/perl.exe")

# Check the dataset
glimpse(fmr2017)

```

```{r, ex1.2-import-xlsx-10}
head (fmr2017)
```


3. Go to this University of Dayton webpage and Download weather data for a selected city.  Download the data for all the Alabama cities.

http://academic.udayton.edu/kissock/http/Weather/citylistUS.htm


```{r, ex1.3-import-weather-data}

# First let's get all the links on the page
url <- "http://academic.udayton.edu/kissock/http/Weather/citylistUS.htm"

links <- getHTMLLinks(url)

# Check the first ten links
head(links, 10)

```

By looking at the webpage we can see that all the extensions are txt time series data.  

All the cities are coded using the two letter abbreviation of the state as prefix, so we can use regular expression to identify those that correspond to Alabama.  Prefix = "AL"


```{r, ex1.3-import-weather-data-10}

# Extract the links corresponding to the four cities in Alabama
linksDataAl <- links[str_detect(links, "^(.+)/AL.+")]

# Check the extraction
linksDataAl

```

```{r, ex1.3-import-weather-data-20}

# Let's get the data into the R session

for (i in seq_along(linksDataAl)) {
  url <- paste0("http://academic.udayton.edu/kissock/http/Weather/",linksDataAl[i])
  df <- read.csv(url, header = FALSE, sep = "")
  assign(paste0("df", i), df)
}

# Let's check the files exists in our R session
sapply(paste0("df", 1:4), exists)

```

```{r, ex1.3-import-weather-data-30}

# Let's see one of the datasets

glimpse(df1)

```

```{r}
head(df1)
```


# 2. Scraping HTML Text

The basis of webscraping will be done attacking these basics elements of which web pages are made of:


    *  <h1>, <h2>,.,<h6>: Largest heading, second largest heading, etc.
    *  <p>: Paragraph elements
    *  <ul>: Unordered bulleted list
    *  <ol>: Ordered list
    *  <li>: Individual List item
    *  <div>: Division or section
    *  <table>: Table

Paragraphs in an HTML document can be identified with the <p>.  It is through these tags that we can start extracting information via scraping.  These tags are also called nodes.

The web page to disect using this example is the [Wikipedia page on web scraping](https://en.wikipedia.org/wiki/Web_scraping).  We will use for this url the variable:
`r wikiUrl <- "https://en.wikipedia.org/wiki/Web_scraping"`

*How to scrape HTML nodes*

The package to use is rvest.

To extract text from a webpage we specify what HTML elements we want to select by using html_node().  

For example, if we want to scrape the heading of the Wikipedia page, we select it identifying the all the `h1` nodes on the webpage.  

```{r, identify-h1-sample-page}

# Load the rvest package


# We need to read in the webpage using read_html()
scrapingWiki <- read_html("https://en.wikipedia.org/wiki/Web_scraping")

scrapingWiki %>% 
  html_nodes("h1")


```

To extract only the text and not all the HTML syntax we use html_text()


```{r, identify-hi-sample-page-10}
scrapingWiki %>% 
  html_nodes("h1") %>% 
  html_text()

```

To identify all the second level headings, we follow the same process.  In this case there 8 second level headings <h2>

```{r, identify-h2-sample-page-10}
scrapingWiki %>% 
  html_nodes("h2") %>% 
  html_text()


```

Now let's select all the paragraph nodes.  How many paragraph does this articl have?

```{r, identify-p-sample-page-10}
# We will assign a varible called pNodes

pNodes <- scrapingWiki %>% 
  html_nodes("p")

length(pNodes)

```

Let's show the first elements of pNodes (the first 6 paragraphs)


```{r, identify-p-sample-page-20}
head(pNodes)


```


Let's select the text only without the HTML syntax

```{r, identify-p-sample-page-30}

pText <- pNodes %>% 
          html_text()
head(pText)

```

An important note, this has only captured the text in paragraph, not the bulleted lists.  Just as the h1 example only captured the heading nodes.  

Items in lists are tagged `ul` 

```{r, identify-ul-sample-page}
ulText <- scrapingWiki %>% 
              html_nodes("ul") %>% 
              html_text()
              

length(ulText)

```

Let's identify the first list which is the one of the table of contents

```{r, identify-ul-sample-page-10}
ulText[1]


```

If we want to read the first 200 characters of a list within a webpage:

```{r, identify-ul-sample-page-20}
# We use the substr() function
substr(ulText[2], start = 1, stop = 200)

```

There is a difference between: ul, ol, il.  ul is for unordered lists, that is the bullet points.  ol is for the numbered lists, ordered.  li is for individual items with the lists.

```{r, identify-li-sample-page}
liText <- scrapingWiki %>% 
            html_nodes("li") %>% 
            html_text()

length(liText)



```

One way to capture all text (regardless of node) within the page is using the `div`

```{r, identify-div-sample-page}
allText <- scrapingWiki %>% 
            html_nodes("div") %>% 
            html_text()

head(allText)

```


*Scraping Specific HTML Nodes*

To scrape specific content we need to be more focused in our HTML selection process.

We use selectorgadget to identify which parts to include in our code.


```{r, select-specific-html-nodes}

bodyText <- scrapingWiki %>% 
              html_nodes(".mw-parser-output") %>% 
              html_text()

# read the first 207 characters
substr(bodyText, start = 1, stop = 500)
```


```{r, select-specific-html-nodes-10}

# read the last 500 characters
substr(bodyText, star = nchar(bodyText) - 550, stop = nchar(bodyText))

```


Selecting the part we are interested in we can select only the elements we require for instance.

```{r, select-specific-html-nodes-20}
# Scraping a specifc heading
scrapingWiki %>% 
  html_nodes("#Techniques") %>% 
  html_text()

```

Number of characters of previous element: `r scrapingWiki %>%   html_nodes("#Techniques") %>%   html_text() %>%   nchar()`

```{r, select-specific-html-nodes-30}
# Scraping a specific paragraph.  better with the code Inspector in the development tools.
scrapingWiki %>% 
  html_nodes(".mw-parser-output > p:nth-child(13)") %>% 
  html_text()

```

```{r, select-specific-html-nodes-40}
# Scraping a specific list
scrapingWiki %>% 
  html_nodes(".div-col") %>% 
  html_text()

```

```{r, select-specific-html-nodes-50}
# Scraping a specific reference list item
scrapingWiki %>% 
  html_nodes("#cite_note-22") %>% 
  html_text()

```

*Cleaning up*

Webscraping yields results that require cleaning (as with every action at this stage in the data collection workflow)

For example: the list in See also is scraped the following way:


```{r, cleaning-up}
scrapingWiki %>% 
  html_nodes(".div-col") %>% 
  html_text()
```

The items are separated by `\n` which HTML that identifies a new line.

This can be cleaned using string manipulation

```{r, cleaning-up-10}
# We will use strsplit to separarte the itmes by \n
scrapingWiki %>% 
  html_nodes(".div-col") %>% 
  html_text() %>% 
  strsplit(split = "\n") %>% 
  unlist() %>% 
  .[.!= ""]       # This removes a set of quotation marks ("") that appear first on list

```

















































































































