---
title: "Scraping HTML Basic"
output:
  html_document:
    toc: yes
    toc_depth: 2
    toc_float: yes
  pdf_document:
    toc: yes
  word_document:
    toc: yes
urlcolor: blue
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE)

```


```{r, load-preliminary-libraries, include = FALSE}
library(dplyr)
library(ggplot2)
library(pander)
library(rvest)
library(XML)
library(stringr)


```


# Basis of this tutorial.  Objectives

This tutorial is basic and will focus on the following tasks:

1. Importing Excel files found online
2. Reading HTML text
3. Reading HTML tables
4. Handling APIs

# 1. Importing Excel files found online

The first task is to download the csv of the federal agencies that supply data to the [Data.gov](https://www.data.gov/metrics)

This is not considered sometimes webscraping at all.

Since lots of data is stored in csv/excel or other tabular formula, a good starting point is to be able to import those from the sites that hosts them.

```{r, import-datagov-metrics-data-csv}

# the url for the online csv
url <- "https://s3.amazonaws.com/bsp-ocsit-prod-east-appdata/datagov/wordpress/agency-participation.csv"

# use read.csv to import
datagov <- read.csv(url, stringsAsFactors = FALSE)

# check the set has been read properly, check first rows
head(datagov)
```

```{r, import-datagov-metrics-data-csv-1}
# Check data summaries, etc.

glimpse(datagov)

```

As can be seen some entries will have to be formatted as factor (Organization.Type), and as Date (Last.Entry)

That was for importing csv files.  Now for importing Excel files.  There is not a base-R function, so a package must be included.  The package `gdata` for instance has a read.xls function. 

The objective is to download the excel file of the Fair Market Rents (FMR)

```{r, import-FMR-excel}

# Load the library that will handle the excel file.  In the tutorial they use gdata.  I will use xlsx since it can handle both xls and xlsx data.  I don't know if gdata handles xlsx.

library(gdata)


url <- "http://www.huduser.org/portal/datasets/fmr/fmr2015f/FY2015F_4050_Final.xls"
  
rents<- read.xls(url, perl = "C:/Perl64/bin/perl.exe")

head(rents)



```



Another common form of file storage is zip files.  For this we can use download.file() to get into our directory and use as desired.


```{r, import-zip-bureau-labor-stats}

# These are the diary of 2014.  The diary of 2016 is available as well.

url <- "http://www.bls.gov/cex/pumd/data/comma/diary14.zip"

# downloado .zip files and unzip

download.file(url, dest = "data/dataset.zip", mode = "wb")

# unzip in same folder.  The exdir must be the name of the folder where it will be unzipped.  Better do it ahead of time in the meantime.
unzip("data/dataset.zip", exdir = "data")

#Check the files within the folder

zipFiles <- list.files("data/diary14")

 # How many files?
length(zipFiles)

```

```{r, import-zip-bureau-labor-stats-1}
# List the files
zipFiles
```


If we know the file we want prior to unzipping we could extract without unzipping the whole set

```{r, import-zip-bureau-labor-stats-2}
zipData <- read.csv(unz("data/dataset.zip", "diary14/expd141.csv"))

glimpse(zipData)

```


```{r, import-zip-bureau-labor-stats-3}

head(zipData)

```

Sometimes the zip file is so big it is counterproductive to save it when all we need to work with is one of the internal files (or a group of them), the proposal is to temporarily save the zip, fetch the files we need and then discard the zip.

```{r, import-zip-bureau-labor-stats-4}

# Create a temp. file name
temp <- tempfile()

# Download the zip to the temp file name.  for url, see item 102
download.file(url, temp)

# Use unz() to extract the target file from temp.
zipData2 <- read.csv(unz(temp, "diary14/expd141.csv"))

# Remove the temp file via unlink()
unlink(temp)


# Check zipData2
glimpse(zipData2)


```

```{r, import-zip-bureau-labor-stats-5}
# Check the dataset
head(zipData2)
```


```{r, import-zip-bureau-labor-stats-6}
# Save it into the working dir
write.csv(zipData2, "data/zipData2.csv")

```

One thing is to detect all the links in a webpage using the XML package getHTMLLinks() function.

```{r, identify-links-in-webpage}

# Load the XML library
library(XML)

# The url to use is the Maryland State Board of Elections website
url <- "http://www.elections.state.md.us/elections/2012/election_data/index.html"
links <- getHTMLLinks(url)

# How many links were fetched?
length(links)


```

```{r, identify-links-in-webpage-1}
# Check links set
head(links)
```


Let's identify all of the links with the .csv extension

```{r, identify-links-in-webpage-csv}
# We will use regular expressions to identify those links that contain csv
# We will require the stingr package
library(stringr)

# extract the names of desired links with csv extension
linksData <- links[str_detect(links, ".csv")]

# How many were identified?
length(linksData)


```


```{r, identify-links-in-webpage-csv-10}
head(linksData, 10)
```

We will download only the first 10 csv files

```{r, identify-links-in-webpage-download-10-csv}

# We will use a for loop.  But first reduce linksData length to 10
linksData <- head(linksData, 10)

for (i in seq_along(linksData)) {
  # step 1: paste .csv portion of link to base URL of page
  url <- paste0("http://www.elections.state.md.us/elections/2012/election_data/",
                linksData[i])
  
  # setp 2: download .csv file and save as df
  df <- read.csv(url)
  
  # step 3: rename df
  assign(paste0("df", i), df)
}

```

Let's check if the 10 df sets exists


```{r, check-dfs}
sapply(paste0("df", 1:10), exists)

```

```{r, check-df8}

head(df8)

```

# Excercises 1

1. Import the following csv file directly from this url: https://bradleyboehmke.github.io/public/data/reddit.csv.


```{r, ex1.1-import-csv}

url <- "https://bradleyboehmke.github.io/public/data/reddit.csv"

reddit <- read.csv(url, stringsAsFactors = FALSE)

# Check the data set
glimpse(reddit)

```


```{r, ex1-import-csv-10}
# Let's save it to our data file
reddit <- write.csv(reddit, "data/reddit.csv")

```


2. Import the .xlsx file directly from this url: http://www.huduser.gov/portal/datasets/fmr/fmr2017/FY2017_4050_FMR.xlsx.

```{r, ex1.2-import-xlsx}

url <- "http://www.huduser.gov/portal/datasets/fmr/fmr2017/FY2017_4050_FMR.xlsx"

fmr2017 <- read.xls(url, perl = "C:/Perl64/bin/perl.exe")

# Check the dataset
glimpse(fmr2017)

```

```{r, ex1.2-import-xlsx-10}
head (fmr2017)
```


3. Go to this University of Dayton webpage and Download weather data for a selected city.  Download the data for all the Alabama cities.

http://academic.udayton.edu/kissock/http/Weather/citylistUS.htm


```{r, ex1.3-import-weather-data}

# First let's get all the links on the page
url <- "http://academic.udayton.edu/kissock/http/Weather/citylistUS.htm"

links <- getHTMLLinks(url)

# Check the first ten links
head(links, 10)

```

By looking at the webpage we can see that all the extensions are txt time series data.  

All the cities are coded using the two letter abbreviation of the state as prefix, so we can use regular expression to identify those that correspond to Alabama.  Prefix = "AL"


```{r, ex1.3-import-weather-data-10}

# Extract the links corresponding to the four cities in Alabama
linksDataAl <- links[str_detect(links, "^(.+)/AL.+")]

# Check the extraction
linksDataAl

```

```{r, ex1.3-import-weather-data-20}

# Let's get the data into the R session

for (i in seq_along(linksDataAl)) {
  url <- paste0("http://academic.udayton.edu/kissock/http/Weather/",linksDataAl[i])
  df <- read.csv(url, header = FALSE, sep = "")
  assign(paste0("df", i), df)
}

# Let's check the files exists in our R session
sapply(paste0("df", 1:4), exists)

```

```{r, ex1.3-import-weather-data-30}

# Let's see one of the datasets

glimpse(df1)

```

```{r}
head(df1)
```


# 2. Scraping HTML Text

The basis of webscraping will be done attacking these basics elements of which web pages are made of:


    *  <h1>, <h2>,.,<h6>: Largest heading, second largest heading, etc.
    *  <p>: Paragraph elements
    *  <ul>: Unordered bulleted list
    *  <ol>: Ordered list
    *  <li>: Individual List item
    *  <div>: Division or section
    *  <table>: Table

Paragraphs in an HTML document can be identified with the <p>.  It is through these tags that we can start extracting information via scraping.  These tags are also called nodes.

The web page to disect using this example is the [Wikipedia page on web scraping](https://en.wikipedia.org/wiki/Web_scraping).  We will use for this url the variable:
`r wikiUrl <- "https://en.wikipedia.org/wiki/Web_scraping"`

*How to scrape HTML nodes*

The package to use is rvest.

To extract text from a webpage we specify what HTML elements we want to select by using html_node().  

For example, if we want to scrape the heading of the Wikipedia page, we select it identifying the all the `h1` nodes on the webpage.  

```{r, identify-h1-sample-page}

# Load the rvest package


# We need to read in the webpage using read_html()
scrapingWiki <- read_html("https://en.wikipedia.org/wiki/Web_scraping")

scrapingWiki %>% 
  html_nodes("h1")


```

To extract only the text and not all the HTML syntax we use html_text()


```{r, identify-hi-sample-page-10}
scrapingWiki %>% 
  html_nodes("h1") %>% 
  html_text()

```

To identify all the second level headings, we follow the same process.  In this case there 8 second level headings <h2>

```{r, identify-h2-sample-page-10}
scrapingWiki %>% 
  html_nodes("h2") %>% 
  html_text()


```

Now let's select all the paragraph nodes.  How many paragraph does this articl have?

```{r, identify-p-sample-page-10}
# We will assign a varible called pNodes

pNodes <- scrapingWiki %>% 
  html_nodes("p")

length(pNodes)

```

Let's show the first elements of pNodes (the first 6 paragraphs)


```{r, identify-p-sample-page-20}
head(pNodes)


```


Let's select the text only without the HTML syntax

```{r, identify-p-sample-page-30}

pText <- pNodes %>% 
          html_text()
head(pText)

```

An important note, this has only captured the text in paragraph, not the bulleted lists.  Just as the h1 example only captured the heading nodes.  

Items in lists are tagged `ul` 

```{r, identify-ul-sample-page}
ulText <- scrapingWiki %>% 
              html_nodes("ul") %>% 
              html_text()
              

length(ulText)

```

Let's identify the first list which is the one of the table of contents

```{r, identify-ul-sample-page-10}
ulText[1]


```

If we want to read the first 200 characters of a list within a webpage:

```{r, identify-ul-sample-page-20}
# We use the substr() function
substr(ulText[2], start = 1, stop = 200)

```

There is a difference between: ul, ol, il.  ul is for unordered lists, that is the bullet points.  ol is for the numbered lists, ordered.  li is for individual items with the lists.

```{r, identify-li-sample-page}
liText <- scrapingWiki %>% 
            html_nodes("li") %>% 
            html_text()

length(liText)



```

One way to capture all text (regardless of node) within the page is using the `div`

```{r, identify-div-sample-page}
allText <- scrapingWiki %>% 
            html_nodes("div") %>% 
            html_text()

head(allText)

```


*Scraping Specific HTML Nodes*

To scrape specific content we need to be more focused in our HTML selection process.

We use selectorgadget to identify which parts to include in our code.


```{r, select-specific-html-nodes}

bodyText <- scrapingWiki %>% 
              html_nodes(".mw-parser-output") %>% 
              html_text()

# read the first 207 characters
substr(bodyText, start = 1, stop = 500)
```


```{r, select-specific-html-nodes-10}

# read the last 500 characters
substr(bodyText, star = nchar(bodyText) - 550, stop = nchar(bodyText))

```


Selecting the part we are interested in we can select only the elements we require for instance.

```{r, select-specific-html-nodes-20}
# Scraping a specifc heading
scrapingWiki %>% 
  html_nodes("#Techniques") %>% 
  html_text()

```

Number of characters of previous element: `r scrapingWiki %>%   html_nodes("#Techniques") %>%   html_text() %>%   nchar()`

```{r, select-specific-html-nodes-30}
# Scraping a specific paragraph.  better with the code Inspector in the development tools.
scrapingWiki %>% 
  html_nodes(".mw-parser-output > p:nth-child(13)") %>% 
  html_text()

```

```{r, select-specific-html-nodes-40}
# Scraping a specific list
scrapingWiki %>% 
  html_nodes(".div-col") %>% 
  html_text()

```

```{r, select-specific-html-nodes-50}
# Scraping a specific reference list item
scrapingWiki %>% 
  html_nodes("#cite_note-22") %>% 
  html_text()

```

*Cleaning up*

Webscraping yields results that require cleaning (as with every action at this stage in the data collection workflow)

For example: the list in See also is scraped the following way:


```{r, cleaning-up}
scrapingWiki %>% 
  html_nodes(".div-col") %>% 
  html_text()
```

The items are separated by `\n` which HTML that identifies a new line.

This can be cleaned using string manipulation

```{r, cleaning-up-10}
# We will use strsplit to separarte the itmes by \n
scrapingWiki %>% 
  html_nodes(".div-col") %>% 
  html_text() %>% 
  strsplit(split = "\n") %>% 
  unlist() %>% 
  .[.!= ""]       # This removes a set of quotation marks ("") that appear first on list

```

When scraping the main body (bodyText) there were many characters that made the text messy.  This can be cleaned up using regex so it consists of only text.

```{r, cleaning-up-20}
# We will use the stringr package.

# read the last 700 characters of bodyText
substr(bodyText, start = nchar(bodyText)- 700, stop = nchar(bodyText))


```

The characters that have to removed and substituted by a " " are:
 * `\n`
 * `\"`
 * `^`


```{r, cleaning-up-30}

# Cleaning up

body700 <- bodyText %>% 
  str_replace_all("\n", " ") %>% 
  str_replace_all("[\\^]", " ") %>% 
  str_replace_all("\"", " ") %>% 
  str_trim(side = "both") %>% 
  substr(start = nchar(bodyText) - 700, stop = nchar(bodyText))
  
body700

```


# Excercises 2

Extract all the text from the main body content for the Wikipedia entry on the [War in Afghanistan](https://en.wikipedia.org/wiki/War_in_Afghanistan_(2001%E2%80%932014). Can you scrape just the references on this page?

```{r, ex2.2-extract-body}

wikiAf <- "https://en.wikipedia.org/wiki/War_in_Afghanistan_(2001%E2%80%932014)"

# read the html of the wiki
scrapewikiAf <- read_html(wikiAf)

# Let's get the text of the wiki main body content
bodyAf <- scrapewikiAf %>% 
  html_nodes("#bodyContent") %>% 
  html_text()

bodyAf

```

```{r, ex2.2-extract-body-ref}
bodyRef <- scrapewikiAf %>% 
            html_nodes("div.reflist:nth-child(451)") %>% 
            html_text()

bodyRef

```

Now let's clean this list of references.  But first let's get the list of references.

```{r, ex2.2-extract-body-ref-10}
bodyRefClean <- bodyRef %>% 
                  strsplit(split = "\n") %>% 
                  unlist() %>% 
                  str_replace_all("\\^", "") %>% 
                  str_replace_all("\"", "") %>% 
                  str_trim(side = "both") %>% 
                  .[. != ""]

bodyRefClean[1:10]

```

# Scraping HTML Table Data

The basics of scraping table data is via the rvest package and the XML package.

The focus will be the [Bureau of Labor Statistics Employment Webpage.](https://www.bls.gov/web/empsit/cesbmart.htm)

The tag or node in HTML for tables is `<table>`.  In the html_nodes() function we select the nodes with table tags.

Let's get the tables found on https://www.bls.gov/web/empsit/cesbmart.htm


```{r, scrape-tables-10}

# First let's read in the webpage
webpage <- read_html("https://www.bls.gov/web/empsit/cesbmart.htm")

# identify or fetch all of the table using html_nodes()
tbls <- webpage %>% 
          html_nodes("table")

head(tbls)

```

html_nodes() function does not parse the data.  It is only a CSS selector.  In order to parse we need to use the html_table() function.

There are two ways to do this with this example.  With tbls, we can select the tables we want by subsetting the vector.  Say Tables2 and Table3 are indexes [3] and [4] respectively.

```{r, scrape-tables-20}
tblsLis <- webpage %>% 
              html_nodes("table") %>% 
              .[3:4] %>% 
              html_table(fill = TRUE)

glimpse(tblsLis)

```

Another way is to parse each table separately.

```{r, scrape-tables-30}
table2 <- webpage %>% 
            html_nodes("#Table2") %>% 
            html_table(fill = TRUE)

glimpse(table2)
table2


```


```{r, scrape-tables-40}
table3 <- webpage %>% 
            html_nodes("#Table3") %>% 
            html_table(fill = TRUE)

glimpse(table3)
table3


```

Let's fix table2 that has heading repeated due to running headings in original table

```{r}
head(table2[[1]], 4)


# remove row 1 that includes part of the headings
table2[[1]] <- table2[[1]][-1,]

# rename table headings
colnames(table2[[1]]) <- c("CES_Code", "Ind_Title", "Benchmark",
                            "Estimate", "Amt_Diff", "Pct_Diff")

head(table2[[1]], 4)
```


####### THE EXCERCISES ARE PENDING TO BE DONE######


# Leveraging APIs to scrape Data

API: Application programming interface is a method of communicating between software and program.

Some pointers with working with API:
a.  The URL of the organization data is being pulled from
b.  The data to be pulled.
c.  The data content.

To access the data there is a form of identification and authorization

a. API key or token
b. OAuth is the form of authorization process

The key rpackage here is the `httr` package.


Before dwelling into an API, it is good to check whether there is a package in R for that API. 

The blsAPI is the API for the Bureau of Labor Statistics in the USA.  No key / token or OAuth is required.  Only knowledge on the data.

For this we will to load or install the corresponding packages throughout the example

*Bureau of Labour API example*

```{r, include=FALSE}
# Load the corresponding packages
library(rjson)
library(blsAPI)

```

It is important to read the documentation on how to pull the data.  For the blsAPI an understanding of the series identifier is important because it tells you which code represent regions and which parts represent demographics or industries.

In this example we are interested in layoffs

```{r, example-blsAPI-layoffs}
# This will pull in data of layoffs for all the US (SOO) across all industries (N0001)

layoffsJson <- blsAPI("MLUMS00NN0001003") # This returns a JSON object

layoffs <- fromJSON(layoffsJson) # This returns an r object, a list, from a Json object

str(layoffs) # In this case str() works better than glimpse()

```


The object has 4 lists and we are interested in the list that says data.  To extract the information and convert it to a table.

```{r, example-blsAPI-layoffs-10}
# First let's create an empty data frame so we can save the structure

layoff_df <- data.frame(NULL)

# Extract the data of interest from each nested year-month list.

for(i in seq_along(layoffs$Results$series[[1]]$data)) {
        df <- data.frame(layoffs$Results$series[[1]]$data[i][[1]][1:4])
        layoff_df <- rbind(layoff_df, df)
}


layoff_df

```

The dataset retrieved requires cleaning and preprocessing but it is captured.


*NOAA Example*

Let's load the NOAA r package.  In this case the key or token is emailed prior requests in their page:
http://www.ncdc.noaa.gov/cdo-web/token

The data to pull is the data sent from the climate stations in a given area provided a FIPS code is passed on the function `ncdc_stations` , and a dataset is selected.  In this case the dataset from the GHCND is going to be used to get the information from the county of the author of the tutorial.

```{r example-noaa}

# Load the library
library(rnoaa)

stations <- ncdc_stations(datasetid = "GHCND",
                          locationid = "FIPS:39113",
                          token = "AdwtGjiCyfGNMDfJNNocadAVeLHtmUqX") # token must be in ""

stations # returns a list where the fourth element is a data table very well structured indeed.
```


```{r, example-noaa-10}

# Let's select only the data item
stations$data

```

I am going to pull the data from Wake County north caroline fips 37,183


```{r, example-noaa-wake-county-nc}

stationsWC <- ncdc_stations(datasetid = "GHCND",
                          locationid = "FIPS:37183",
                          token = "AdwtGjiCyfGNMDfJNNocadAVeLHtmUqX")

stationsWC$data  
```

```{r, example-noaa-20}

# Let's arrange the data so that I see which lattitude is closest to leesville Road where my sister lives
# 35.870678


stationsWC$data %>% 
  arrange(latitude)
```

The answer is `35.8624	RALEIGH 5.9 ENE, NC US`.  So I need to get the id of this station to query it.

```{r, example-noaa-30}

stationsWC$data %>% 
  filter(name == "RALEIGH 5.9 ENE, NC US") %>% 
  select(mindate, maxdate, id)

```


To get the data from a staion we simply pull the ncdc() function with the information in the previous result (item 881) and the dataset needed: the GHCND

```{r, example-noaa-40}
climate <- ncdc(datasetid = "GHCND",
                startdate = "2017-10-05",
                enddate = "2017-11-12",
                stationid = "GHCND:US1NCWK0013",
                token = "AdwtGjiCyfGNMDfJNNocadAVeLHtmUqX")

```



*nytimes API*

The newyork times api offers a lots of api to choose from.  PENDING.






































































































































































