---
title: "Scraping HTML"
output: html_notebook
---

---
title: "Scraping HTML Basic"
output:
  pdf_document:
    toc: yes
  html_document:
    toc: yes
    toc_depth: 2
    toc_float: yes
  word_document:
    toc: yes
urlcolor: blue
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE)

```


```{r, load-preliminary-libraries, include = FALSE}
library(dplyr)
library(ggplot2)


```


# Basis of this tutorial.  Objectives

This tutorial is basic and will focus on the following tasks:

1. Importing Excel files found online
2. Reading HTML text
3. Reading HTML tables
4. Handling APIs

# 1. Importing Excel files found online

The first task is to download the csv of the federal agencies that supply data to the [Data.gov](https://www.data.gov/metrics)

This is not considered sometimes webscraping at all.

Since lots of data is stored in csv/excel or other tabular formula, a good starting point is to be able to import those from the sites that hosts them.

```{r, import-datagov-metrics-data-csv}

# the url for the online csv
url <- "https://s3.amazonaws.com/bsp-ocsit-prod-east-appdata/datagov/wordpress/agency-participation.csv"

# use read.csv to import
datagov <- read.csv(url, stringsAsFactors = FALSE)

# check the set has been read properly, check first rows
head(datagov)
```

```{r, import-datagov-metrics-data-csv-1}
# Check data summaries, etc.

glimpse(datagov)

```

As can be seen some entries will have to be formatted as factor (Organization.Type), and as Date (Last.Entry)

That was for importing csv files.  Now for importing Excel files.  There is not a base-R function, so a package must be included.  The package `gdata` for instance has a read.xls function. 

The objective is to download the excel file of the Fair Market Rents (FMR)

```{r, import-FMR-excel}

# Load the library that will handle the excel file.  In the tutorial they use gdata.  I will use xlsx since it can handle both xls and xlsx data.  I don't know if gdata handles xlsx.

library(gdata)


url <- "http://www.huduser.org/portal/datasets/fmr/fmr2015f/FY2015F_4050_Final.xls"
  
rents<- read.xls(url, perl = "C:/Perl64/bin/perl.exe")

head(rents)



```



Another common form of file storage is zip files.  For this we can use download.file() to get into our directory and use as desired.


```{r, import-zip-bureau-labor-stats}

# These are the diary of 2014.  The diary of 2016 is available as well.

url <- "http://www.bls.gov/cex/pumd/data/comma/diary14.zip"

# downloado .zip files and unzip

download.file(url, dest = "data/dataset.zip", mode = "wb")

# unzip in same folder.  The exdir must be the name of the folder where it will be unzipped.  Better do it ahead of time in the meantime.
unzip("data/dataset.zip", exdir = "data")

#Check the files within the folder

zipFiles <- list.files("data/diary14")

 # How many files?
length(zipFiles)

```

```{r, import-zip-bureau-labor-stats-1}
# List the files
zipFiles
```


If we know the file we want prior to unzipping we could extract without unzipping the whole set

```{r, import-zip-bureau-labor-stats-2}
zipData <- read.csv(unz("data/dataset.zip", "diary14/expd141.csv"))

glimpse(zipData)

```


```{r, import-zip-bureau-labor-stats-3}

head(zipData)

```

Sometimes the zip file is so big it is counterproductive to save it when all we need to work with is one of the internal files (or a group of them), the proposal is to temporarily save the zip, fetch the files we need and then discard the zip.

```{r, import-zip-bureau-labor-stats-4}

# Create a temp. file name
temp <- tempfile()

# Download the zip to the temp file name.  for url, see item 102
download.file(url, temp)

# Use unz() to extract the target file from temp.
zipData2 <- read.csv(unz(temp, "diary14/expd141.csv"))

# Remove the temp file via unlink()
unlink(temp)


# Check zipData2
glimpse(zipData2)


```

```{r, import-zip-bureau-labor-stats-5}
# Check the dataset
head(zipData2)
```


```{r, import-zip-bureau-labor-stats-6}
# Save it into the working dir
write.csv(zipData2, "data/zipData2.csv")

```

One thing is to detect all the links in a webpage using the XML package getHTMLLinks() function.

```{r, identify-links-in-webpage}

# Load the XML library
library(XML)

# The url to use is the Maryland State Board of Elections website
url <- "http://www.elections.state.md.us/elections/2012/election_data/index.html"
links <- getHTMLLinks(url)

# How many links were fetched?
length(links)


```

```{r, identify-links-in-webpage-1}
# Check links set
head(links)
```


Let's identify all of the links with the .csv extension

```{r, identify-links-in-webpage-csv}
# We will use regular expressions to identify those links that contain csv
# We will require the stingr package
library(stringr)

# extract the names of desired links with csv extension
linksData <- links[str_detect(links, ".csv")]

# How many were identified?
length(linksData)


```


```{r, identify-links-in-webpage-csv-10}
head(linksData, 10)
```

We will download only the first 10 csv files

```{r, identify-links-in-webpage-download-10-csv}

# We will use a for loop.  But first reduce linksData length to 10
linksData <- head(linksData, 10)

for (i in seq_along(linksData)) {
  # step 1: paste .csv portion of link to base URL of page
  url <- paste0("http://www.elections.state.md.us/elections/2012/election_data/",
                linksData[i])
  
  # setp 2: download .csv file and save as df
  df <- read.csv(url)
  
  # step 3: rename df
  assign(paste0("df", i), df)
}

```

Let's check if the 10 df sets exists


```{r, check-dfs}
sapply(paste0("df", 1:10), exists)

```

```{r, check-df8}

head(df8)

```

# Excercises 1

1. Import the following csv file directly from this url: https://bradleyboehmke.github.io/public/data/reddit.csv.


```{r, ex1.1-import-csv}

url <- "https://bradleyboehmke.github.io/public/data/reddit.csv"

reddit <- read.csv(url, stringsAsFactors = FALSE)

# Check the data set
glimpse(reddit)

```


```{r, ex1-import-csv-10}
# Let's save it to our data file
reddit <- write.csv(reddit, "data/reddit.csv")

```


2. Import the .xlsx file directly from this url: http://www.huduser.gov/portal/datasets/fmr/fmr2017/FY2017_4050_FMR.xlsx.

```{r, ex1.2-import-xlsx}

url <- "http://www.huduser.gov/portal/datasets/fmr/fmr2017/FY2017_4050_FMR.xlsx"

fmr2017 <- read.xls(url, perl = "C:/Perl64/bin/perl.exe")

# Check the dataset
glimpse(fmr2017)

```

```{r, ex1.2-import-xlsx-10}
head (fmr2017)
```


3. Go to this University of Dayton webpage and Download weather data for a selected city.  Download the data for all the Alabama cities.

http://academic.udayton.edu/kissock/http/Weather/citylistUS.htm


```{r, ex1.3-import-weather-data}

# First let's get all the links on the page
url <- "http://academic.udayton.edu/kissock/http/Weather/citylistUS.htm"

links <- getHTMLLinks(url)

# Check the first ten links
head(links, 10)

```

By looking at the webpage we can see that all the extensions are txt time series data.  

All the cities are coded using the two letter abbreviation of the state as prefix, so we can use regular expression to identify those that correspond to Alabama.  Prefix = "AL"


```{r, ex1.3-import-weather-data-10}

# Extract the links corresponding to the four cities in Alabama
linksDataAl <- links[str_detect(links, "^(.+)/AL.+")]

# Check the extraction
linksDataAl

```

```{r, ex1.3-import-weather-data-20}

# Let's get the data into the R session

for (i in seq_along(linksDataAl)) {
  url <- paste0("http://academic.udayton.edu/kissock/http/Weather/",linksDataAl[i])
  df <- read.csv(url, header = FALSE, sep = "")
  assign(paste0("df", i), df)
}

# Let's check the files exists in our R session
sapply(paste0("df", 1:4), exists)

```

```{r, ex1.3-import-weather-data-30}

# Let's see one of the datasets

glimpse(df1)

```

```{r}
head(df1)
```












